/*
 * Thread-safe bump allocator for per-request arenas.
 *
 * API is intentionally minimal and C11-only. The implementation lives in
 * cc/runtime/arena.c.
 */
#ifndef CC_ARENA_H
#define CC_ARENA_H

#include <stddef.h>
#include <stdint.h>
#include <ccc/cc_slice.cch>
#include <ccc/cc_atomic.cch>

/* Internal macros using cc_atomic interface */
#define CC_ATOMIC_FETCH_ADD(ptr, val) cc_atomic_fetch_add((ptr), (val))
#define CC_ATOMIC_LOAD(ptr) cc_atomic_load((ptr))
#define CC_ATOMIC_STORE(ptr, val) cc_atomic_store((ptr), (val))
#define CC_ATOMIC_CAS(ptr, expected_ptr, desired) cc_atomic_cas((ptr), (expected_ptr), (desired))

typedef struct {
    uint8_t *base;
    size_t capacity;
    cc_atomic_size offset;
    uint64_t provenance;      // monotonically increasing arena id
    uint32_t _reserved;
} CCArena;

typedef struct {
    CCArena* arena;
    size_t offset;
} ArenaCheckpoint;

// Global provenance counter (defined in runtime).
extern cc_atomic_u64 cc_arena_prov_counter;

// Allocation helpers --------------------------------------------------------

static inline size_t cc__align_up(size_t value, size_t align) {
    size_t a = align ? align : sizeof(void *);
    return (value + (a - 1)) & ~(a - 1);
}

// Initialize an arena with caller-provided backing storage.
// Returns 0 on success, non-zero on invalid parameters.
static inline int cc_arena_init(CCArena *arena, void *buffer, size_t capacity) {
    if (!arena || !buffer || capacity == 0) {
        return -1;
    }
    arena->base = (uint8_t *)buffer;
    arena->capacity = capacity;
    CC_ATOMIC_STORE(&arena->offset, 0);
    arena->provenance = CC_ATOMIC_FETCH_ADD(&cc_arena_prov_counter, 1);
    arena->_reserved = 0;
    return 0;
}

// Allocate `size` bytes aligned to `align` (power-of-two, >=1).
// Returns NULL on exhaustion; no automatic growth.
static inline void *cc_arena_alloc(CCArena *arena, size_t size, size_t align) {
    if (!arena || !arena->base || size == 0) {
        return NULL;
    }
    size_t aligned_offset = 0;
    size_t new_offset = 0;
    size_t expected = CC_ATOMIC_LOAD(&arena->offset);
    for (;;) {
        aligned_offset = cc__align_up(expected, align);
        if (aligned_offset > arena->capacity) {
            return NULL;
        }
        new_offset = aligned_offset + size;
        if (new_offset > arena->capacity) {
            return NULL;
        }
        size_t desired = new_offset;
        if (CC_ATOMIC_CAS(&arena->offset, &expected, desired)) {
            // CAS succeeded; expected now holds previous value.
            break;
        }
        // CAS failed; expected updated with current value, retry.
    }
    return arena->base + aligned_offset;
}

// Reset arena to empty. Does not free backing storage.
static inline void cc_arena_reset(CCArena *arena) {
    if (!arena) return;
    CC_ATOMIC_STORE(&arena->offset, 0);
    // Bump provenance so new allocations get a fresh id (helps detect stale use).
    arena->provenance = CC_ATOMIC_FETCH_ADD(&cc_arena_prov_counter, 1);
}

// Capture current arena allocation state.
static inline ArenaCheckpoint arena_checkpoint(CCArena* arena) {
    ArenaCheckpoint cp;
    cp.arena = arena;
    cp.offset = arena ? CC_ATOMIC_LOAD(&arena->offset) : 0;
    return cp;
}

// Restore arena to a previously captured checkpoint.
// Note: This does NOT bump provenance; allocations before the checkpoint remain valid.
static inline void arena_restore(ArenaCheckpoint checkpoint) {
    CCArena* arena = checkpoint.arena;
    if (!arena) return;
    size_t off = checkpoint.offset;
    if (off > arena->capacity) off = arena->capacity;
    CC_ATOMIC_STORE(&arena->offset, off);
}

// Convenience: compute how many bytes remain.
static inline size_t cc_arena_remaining(const CCArena *arena) {
    if (!arena || !arena->base) {
        return 0;
    }
    size_t off = CC_ATOMIC_LOAD(&arena->offset);
    if (arena->capacity < off) return 0;
    return arena->capacity - off;
}

// Allocate a tracked slice of raw bytes from the arena. Returns an empty slice on failure.
static inline CCSlice cc_arena_alloc_slice_bytes(CCArena *arena, size_t len) {
    void *ptr = cc_arena_alloc(arena, len, 1);
    if (!ptr || !arena) {
        return cc_slice_empty();
    }
    uint64_t id = cc_slice_make_id(arena->provenance, false, false, false);
    return cc_slice_from_parts(ptr, len, id, len);
}

// Allocate a tracked slice for `count` elements of size `elem_size`.
static inline CCSlice cc_arena_alloc_slice(CCArena *arena, size_t elem_size, size_t count, size_t align) {
    size_t bytes = elem_size * count;
    void *ptr = cc_arena_alloc(arena, bytes, align ? align : elem_size);
    if (!ptr || !arena) {
        return cc_slice_empty();
    }
    uint64_t id = cc_slice_make_id(arena->provenance, false, false, false);
    // len/alen expressed in element count, per slice ABI convention.
    return cc_slice_from_parts(ptr, count, id, count);
}

/*
 * Spec-aligned type-safe arena allocation macros.
 * Usage:
 *   int* arr = arena_alloc(int, arena, 100);   // allocate 100 ints
 *   char* buf = arena_alloc(char, arena, 256); // allocate 256 chars
 *
 * These are sugar for cc_arena_alloc(arena, count * sizeof(T), alignof(T)).
 */
#define arena_alloc(T, arena, count) \
    ((T*)cc_arena_alloc((arena), (count) * sizeof(T), _Alignof(T)))

/* Single-element version */
#define arena_alloc1(T, arena) arena_alloc(T, arena, 1)

#endif // CC_ARENA_H


